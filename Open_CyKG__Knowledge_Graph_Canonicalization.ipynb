{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Github Open-CyKG_ Knowledge Graph and Canonicalization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN6vDrEC3k8SoB1/QhJPXxT"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl-laMjVY4HR"
      },
      "source": [
        "Imports and Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFWCB5RT_hFl"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#torch.cuda.current_device(),torch.cuda.device_count(),torch.cuda.is_available()\n",
        "!pip install tiny-tokenizer flair\n",
        "!pip install flair\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from flair.embeddings import XLNetEmbeddings\n",
        "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
        "from flair.data import  Sentence, Token\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, \\\n",
        "    CharLMEmbeddings\n",
        "from typing import List\n",
        "import torch\n",
        "import os\n",
        "import re\n",
        "from flair.embeddings import BertEmbeddings\n",
        "from flair.embeddings import XLMRobertaEmbeddings\n",
        "from flair.data import Sentence\n",
        "from flair.embeddings import TransformerWordEmbeddings\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5gOc40dYmhU"
      },
      "source": [
        "Read File (should be a csv/exl with sent, pos, labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IppRiTp6Unzg"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import StanfordTagger\n",
        "import nltk\n",
        "import pandas as pd\n",
        "nltk.download(\"popular\")\n",
        "\n",
        "#df = read your file here \n",
        "words=df['word']\n",
        "allPOS=[]\n",
        "for word in words:\n",
        "  if word==\"--padding--\":\n",
        "    allPOS.append(\"-PAD-\")\n",
        "  elif str(word)==\"nan\":\n",
        "    allPOS.append(\"--Nan--\")\n",
        "  elif word.isdigit():\n",
        "    allPOS.append(\"-NUM-\")\n",
        "  else:  \n",
        "    allPOS.append(nltk.pos_tag([word])[0][1])\n",
        "df['pos']=pd.Series(allPOS)\n",
        "#df.to_csv(save here if you want)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySfqJYUAj-jC"
      },
      "source": [
        "pick either options document embedding or sentence embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4m_ahIocC0V"
      },
      "source": [
        "pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6O80uxNFukB"
      },
      "source": [
        "Word Embedding:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYJrTy_FpEb7"
      },
      "source": [
        "[link to Flairs word embedding documentation](https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/TRANSFORMER_EMBEDDINGS.md), Hugging face link of all pretrained models https://huggingface.co/transformers/v2.3.0/pretrained_models.html\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AEQ2qK-Ftfy"
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.embeddings import TransformerWordEmbeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OSsqT2tjhgG"
      },
      "source": [
        "Document embedding :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZn2ZnK1kWrN"
      },
      "source": [
        " [https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/DOCUMENT_POOL_EMBEDDINGS.md](https://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65sEa-fNiRut"
      },
      "source": [
        "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
        "\n",
        "# initialize the word embeddings\n",
        "glove_embedding = WordEmbeddings('glove')\n",
        "\n",
        "# initialize the document embeddings, mode = mean\n",
        "document_embeddings = DocumentPoolEmbeddings([glove_embedding])\n",
        "# create an example sentence\n",
        "sentence = Sentence('The grass is green . And the sky is blue .')\n",
        "\n",
        "# embed the sentence with our document embedding\n",
        "document_embeddings.embed(sentence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fA-uUno_xuB"
      },
      "source": [
        "example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdDCPev3Yvox"
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.embeddings import SentenceTransformerDocumentEmbeddings\n",
        "\n",
        "# init embedding\n",
        "embedding = SentenceTransformerDocumentEmbeddings('bert-base-nli-mean-tokens')\n",
        "#embedding=SentenceTransformerDocumentEmbeddings('nli-bert-base-max-pooling')\n",
        "\n",
        "# create a sentence\n",
        "sentence = Sentence('The grass is green .')\n",
        "\n",
        "# embed the sentence\n",
        "embedding.embed(sentence)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5u80IS4_3Qz"
      },
      "source": [
        "My  Embedding (word and sentence)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nnVc2ZV01Q9"
      },
      "source": [
        "df2=  pd.DataFrame()\n",
        "words=df['word']\n",
        "labels=df['predicted_label']\n",
        "POS=df['pos']\n",
        "trueLabels=df['true_label']\n",
        "\n",
        "# getting no  of sents\n",
        "sents=0\n",
        "for word in words:\n",
        "  if word=='.':\n",
        "    sents+=1\n",
        "\n",
        "assert(len(words)==len(labels)==len(POS)) #sanity check\n",
        "\n",
        "#for filling original test file\n",
        "allWords=[]\n",
        "allLabels=[]\n",
        "currentSent=0\n",
        "allSentID=[]\n",
        "allPos=[]\n",
        "allTrueLabels=[]\n",
        "#for sentence embedding purposes, \n",
        "currentSentence=[]\n",
        "embedding = SentenceTransformerDocumentEmbeddings('bert-base-nli-mean-tokens')\n",
        "allSentenceEmbeddings=[]\n",
        "mySentenceEmbeddingsDict={}\n",
        "\n",
        "\n",
        "#for loop to fill in the original test file and sentence embedding as well, word-label-SentID-POS-sentenceEmbedding\n",
        "for word,label,pos,tLabel in zip(words,labels,POS,trueLabels):\n",
        "   if word==\"--padding--\" or str(word)==\"nan\":\n",
        "    continue  \n",
        "   else:  \n",
        "     allWords.append(word)\n",
        "     allLabels.append(label)\n",
        "     allPos.append(pos)\n",
        "     allSentID.append(currentSent)\n",
        "     currentSentence.append(word) \n",
        "     allTrueLabels.append(tLabel)\n",
        "     if word!=\".\": \n",
        "          allSentenceEmbeddings.append(\"\")\n",
        "\n",
        "   if word==\".\":\n",
        "    currentSentence=\" \".join(currentSentence)\n",
        "    sentence=Sentence(currentSentence) \n",
        "    embedding.embed(sentence) \n",
        "    allSentenceEmbeddings.append(str(sentence.embedding)) \n",
        "    mySentenceEmbeddingsDict[currentSent]=str(sentence.embedding)\n",
        "    currentSentence=[]\n",
        "    currentSent+=1 \n",
        "\n",
        "embedding = TransformerWordEmbeddings('bert-base-uncased') #or xlnet-base-cased or xlm-roberta-base\n",
        "allWordEmbeddings=[]\n",
        "allWordEmbeddingsTensors=[] \n",
        "\n",
        "for word,label,pos in zip(words,labels,POS):\n",
        "   if word==\"--padding--\" or str(word)==\"nan\": \n",
        "    continue  \n",
        "   else:  \n",
        "     currentSentence.append(word)\n",
        "\n",
        "   if word==\".\":\n",
        "    currentSentence=\" \".join(currentSentence)\n",
        "    sentence=Sentence(currentSentence, use_tokenizer=False) \n",
        "    sentTokens=[]\n",
        "    embedding.embed(sentence)\n",
        "    for token in sentence:\n",
        "\n",
        "      sentTokens.append((str(token).split())[-1])\n",
        "      allWordEmbeddings.append(str(token.embedding))\n",
        "      allWordEmbeddingsTensors.append(token.embedding)\n",
        "    sentTokens=\" \".join(sentTokens)\n",
        "    currentSentence=[]\n",
        "    sentTokens=[]\n",
        "    currentSent+=1 \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-X1pzI1ngSA"
      },
      "source": [
        "Utlize output from NER model; if you want to use in fusion process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPhWQiEzCH05"
      },
      "source": [
        "df['finalSentID']=df['originalSentID']\n",
        "df['FinalWord']=df['NERword']\n",
        "\n",
        "Ner1=df['NERpredicted1']\n",
        "Ner2=df['NERpredicted2']\n",
        "NerFinal=[]\n",
        "\n",
        "for n1,n2 in zip(Ner1,Ner2):\n",
        "  if n1=='O':\n",
        "    NerFinal.append(n2)\n",
        "  else:\n",
        "    NerFinal.append(n1)  \n",
        "\n",
        "df['finalNER']=pd.Series(NerFinal)\n",
        "df['finalOIELabel']=df['originalLabels']\n",
        "df['finalPOS']=df['originalPOS']\n",
        "\n",
        "df.to_csv(\"/content/gdrive/MyDrive/my Personal work/Open-CyKG/MalwareDB_dataset_csv_exl/OIEoutput(10Feb).csv\", index=False)# float_format=\"%.10g\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5j7a3dqgdR3"
      },
      "source": [
        "--> create CSV output to use with NEO4J or your choice of Graph software"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26IJKD0Ul4WN"
      },
      "source": [
        "#imports and read file\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import pandas as pd\n",
        "# df=read your file here\n",
        "w1=allWordEmbeddingsTensors[0]\n",
        "w2=allWordEmbeddingsTensors[1]\n",
        "w1Plusw2 = w1.add(w2)\n",
        "avg=w1Plusw2/2\n",
        "#df3=read file here\n",
        "\n",
        "# This is a class te get sentence. The each sentence will be list of tuples with its tag and pos.\n",
        "class sentence(object):\n",
        "    def __init__(self, df):\n",
        "        self.n_sent = 1\n",
        "        self.df = df\n",
        "        self.empty = False\n",
        "        agg = lambda s : [(w, p, t,l) for w, p, t, l in zip(s['FinalWord'].values.tolist(),\n",
        "                                                       s['finalPOS'].values.tolist(),\n",
        "                                                       s['finalNER'].values.tolist(),\n",
        "                                                       s['finalTrueLabel'].values.tolist())] #injy\n",
        "        self.grouped = self.df.groupby(\"finalSentID\").apply(agg) #injy\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "        \n",
        "    def get_text(self):\n",
        "        try:\n",
        "            s = self.grouped[self.n_sent]\n",
        "            self.n_sent +=1\n",
        "            return s\n",
        "        except:\n",
        "            return None\n",
        "getter = sentence(df3) \n",
        "sentences = [\" \".join([str(s[0]) for s in sent]) for sent in getter.sentences]\n",
        "#sentence with its pos and tag.\n",
        "import collections\n",
        "getter = sentence(df3) #injy\n",
        "\n",
        "sent = getter.get_text()\n",
        "\n",
        "sentences = getter.sentences\n",
        "\n",
        "#declare all 3 dataframes that will represent the 3 final CSV files(tables)\n",
        "\n",
        "dfRel=pd.DataFrame()\n",
        "dfE1=pd.DataFrame()\n",
        "dfE2=pd.DataFrame()\n",
        "\n",
        "\n",
        "#Colomns to fill in the table\n",
        "col_E1_ID=[]\n",
        "col_E2_ID=[]\n",
        "col_NameRel=[]\n",
        "col_NameE1=[]\n",
        "col_NameE2=[]\n",
        "col_NER_Rel=[]\n",
        "col_POS_Rel=[]\n",
        "col_NER_E1=[]\n",
        "col_POS_E1=[]\n",
        "col_NER_E2=[]\n",
        "col_POS_E2=[]\n",
        "col_E1Rel_ID=[] \n",
        "col_E2Rel_ID=[]\n",
        "\n",
        "allEntity1Dict={} \n",
        "allEntity2Dict={} \n",
        "\n",
        "sentID=0\n",
        "for s in sentences:\n",
        "    seenTriple=set()\n",
        "    seen2Ent=set()\n",
        "    seenE1ID=set() \n",
        "    seenE2ID=set() \n",
        "    OIELabels=[s[n][3] for n in range(len(s))]\n",
        "    sentWord=[s[n][0] for n in range(len(s))]\n",
        "    sentNER=[s[n][2] for n in range(len(s))]\n",
        "    sentPOS=[s[n][1] for n in range(len(s))]\n",
        "    \n",
        "    e1=[]\n",
        "    e2=[]\n",
        "    rel=[]\n",
        "    Labels=#Add list of labels here\n",
        "    i=0\n",
        "    eID=sentID \n",
        "    while i<len(s):\n",
        "      label=OIELabels[i]\n",
        "      if label=='O':\n",
        "        i+=1\n",
        "        continue\n",
        "      if 'Action' in label or 'Modifier' in label:\n",
        "        rel=[]\n",
        "        if 'B' in label: \n",
        "            rel.append(str(i))\n",
        "            i+=1\n",
        "            while (OIELabels[i] in [Labels]) and i < len(s):\n",
        "              rel.append(str(i))\n",
        "              i+=1\n",
        "      if 'Entity' in label:\n",
        "        if 'B' in label and not e1: \n",
        "          e1.append(str(i))\n",
        "          e1ID=round(eID+0.1,1)  \n",
        "          eID=round(eID+0.1,1)  \n",
        "          i+=1\n",
        "          while OIELabels[i]=='I-Entity' and i < len(s):\n",
        "            e1.append(str(i))\n",
        "            i+=1\n",
        "          laste1=1\n",
        "          \n",
        "        elif 'B' in label and not e2:\n",
        "          e2.append(str(i))\n",
        "          e2ID=round(eID+0.1,1) \n",
        "          eID=round(eID+0.1,1)  \n",
        "          i+=1\n",
        "          while  i < len(s) and OIELabels[i]=='I-Entity':\n",
        "            e2.append(str(i))\n",
        "            i+=1\n",
        "          laste1=0\n",
        "        elif e1 and e2 and laste1: \n",
        "          e2=[]\n",
        "        elif e1 and e2 and not laste1: \n",
        "          e1=[]\n",
        "\n",
        "      if e1 and e2 and rel:\n",
        "          all3=e1+rel+e2\n",
        "          all3=\"\".join(all3)\n",
        "          all3V2=e2+rel+e1\n",
        "          all3V2=\"\".join(all3V2)\n",
        "          TwoEnt=e1+e2  \n",
        "          TwoEnt=\"\".join(TwoEnt)\n",
        "\n",
        "          if all3 not in seenTriple and all3V2 not in seenTriple and TwoEnt not in seen2Ent:\n",
        "                triple={'e1':e1, 'rel': rel, 'e2':e2}\n",
        "                seenTriple.add(all3)\n",
        "                seenTriple.add(all3V2)\n",
        "                seen2Ent.add(TwoEnt)\n",
        "                for key in triple:\n",
        "                  words=POS=NER=\"\"\n",
        "                  if key=='e1' :\n",
        "                    col_E1Rel_ID.append(e1ID)\n",
        "                    if e1ID not in seenE1ID: col_E1_ID.append(e1ID)\n",
        "                    \n",
        "                  elif key=='e2':\n",
        "                    col_E2Rel_ID.append(e2ID)\n",
        "                    if  e2ID not in seenE2ID: col_E2_ID.append(e2ID) \n",
        "\n",
        "                  for value in triple[key]:\n",
        "                    words+=sentWord[int(value)] \n",
        "                    words+=\" \"\n",
        "                    NER+=sentNER[int(value)] \n",
        "                    NER+=\" \"\n",
        "                    POS+=sentPOS[int(value)] \n",
        "                    POS+=\" \"\n",
        "\n",
        "                  if key=='e1' and e1ID not in seenE1ID: \n",
        "                    col_NameE1.append(words)\n",
        "                    col_NER_E1.append(NER) \n",
        "                    col_POS_E1.append(POS) \n",
        "                    seenE1ID.add(e1ID)\n",
        "                  elif key=='e2' and e2ID not in seenE2ID: \n",
        "                    col_NameE2.append(words)\n",
        "                    col_NER_E2.append(NER) \n",
        "                    col_POS_E2.append(POS) \n",
        "                    seenE2ID.add(e2ID)\n",
        "                  elif key=='rel': \n",
        "                    col_NameRel.append(words)\n",
        "                    col_NER_Rel.append(NER) \n",
        "                    col_POS_Rel.append(POS) \n",
        "\n",
        "    sentID+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3WM3_X3BdDk"
      },
      "source": [
        "myE1Dict={}\n",
        "idx=0\n",
        "toDelE1_idx=set()\n",
        "updateE1_inRelDict={}\n",
        "\n",
        "for word,id in zip(col_NameE1,col_E1_ID):\n",
        "     if word in myE1Dict: \n",
        "        toDelE1_idx.add(idx) \n",
        "        updateE1_inRelDict[id]=myE1Dict[word] \n",
        "     else:\n",
        "       myE1Dict[word]=id\n",
        "     idx+=1\n",
        "\n",
        "col_E1_ID=[v for idx,v in enumerate(col_E1_ID) if idx not in toDelE1_idx]\n",
        "col_NameE1=[v for idx,v in enumerate(col_NameE1) if idx not in toDelE1_idx]\n",
        "col_NER_E1 =[v for idx,v in enumerate(col_NER_E1) if idx not in toDelE1_idx]\n",
        "col_POS_E1=[v for idx,v in enumerate(col_POS_E1) if idx not in toDelE1_idx]\n",
        "\n",
        "for idx,v in enumerate(col_E1Rel_ID):\n",
        "     if v in updateE1_inRelDict:\n",
        "       col_E1Rel_ID[idx]=updateE1_inRelDict[v]\n",
        "\n",
        "myE2Dict={}\n",
        "idx=0\n",
        "toDelE2_idx=set()\n",
        "updateE2_inRelDict={}\n",
        "\n",
        "#step 1: identify entity appearning twice or more\n",
        "for word,id in zip(col_NameE2,col_E2_ID):\n",
        "     if word in myE2Dict:\n",
        "        toDelE2_idx.add(idx) \n",
        "        updateE2_inRelDict[id]=myE2Dict[word] \n",
        "     else:\n",
        "       myE2Dict[word]=id\n",
        "     idx+=1\n",
        "\n",
        "#step2: now remove indecees in toDelE1_idx from all Entity 1 table coloumns:\n",
        "\n",
        "col_E2_ID=[v for idx,v in enumerate(col_E2_ID) if idx not in toDelE2_idx]\n",
        "col_NameE2=[v for idx,v in enumerate(col_NameE2) if idx not in toDelE2_idx]\n",
        "col_NER_E2 =[v for idx,v in enumerate(col_NER_E2) if idx not in toDelE2_idx]\n",
        "col_POS_E2=[v for idx,v in enumerate(col_POS_E2) if idx not in toDelE2_idx]\n",
        "\n",
        "\n",
        "#step3 : now Entity1 col in Relation table still has the old entry of the idx, update it from updateE1_inRelDict:\n",
        "for idx,v in enumerate(col_E2Rel_ID):\n",
        "     if v in updateE2_inRelDict:\n",
        "       col_E2Rel_ID[idx]=updateE2_inRelDict[v]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwe3-0JCd1eW"
      },
      "source": [
        " HAC\n",
        "Create clusters for E1 [link ref CESI github](https://github.com/IS5882/cesi/blob/master/src/cluster.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQMssq-evExw"
      },
      "source": [
        "from helper import *\n",
        "from joblib import Parallel, delayed\n",
        "import numpy as np, time, random, pdb, itertools\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from scipy.spatial.distance import pdist\n",
        "n=len(allE1AvgEmbTensors)\n",
        "m=768\n",
        "X \t= np.empty((n, m), np.float32)\n",
        "\n",
        "\n",
        "for i in range(len(allE1AvgEmbTensors)): \n",
        "\t\t\tX[i, :] = allE1AvgEmbTensors[i]\n",
        "\n",
        "dist \t  = pdist(X, \t  metric='cosine')\n",
        "clust_res = linkage(dist, method='complete') \n",
        "labels    = fcluster(clust_res, t=.4329, criterion='distance') - 1\n",
        "clusters  = [[] for i in range(max(labels) + 1)]\n",
        "for i in range(len(labels)): \n",
        "\t\t\tclusters[labels[i]].append(i)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnXYH4tSdjb8"
      },
      "source": [
        "Entity Representive and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn9bqKIDVtkr"
      },
      "source": [
        "import collections\n",
        "weights=collections.Counter()\n",
        "\n",
        "for s in sentences:\n",
        "  sentWord=[s[n][0] for n in range(len(s))]\n",
        "  POS=[s[n][1] for n in range(len(s))]\n",
        "  \n",
        "  for word,pos in zip(sentWord,POS):\n",
        "      weights[word]+=1\n",
        "\n",
        "ent2freq=weights \n",
        "final_res = dict()\n",
        "clusterNo=0\n",
        "cesi_clust2ent_mod2={} \n",
        "index_to_word={}\n",
        "for cluster in clusters:\n",
        "      rep, max_freq = cluster[0], -1\n",
        "      score, maxScore=0, 0\n",
        "      wordList=[]\n",
        "      for ent in cluster:\n",
        "        words=col_NameE1[ent]\n",
        "        wordList.append(words)\n",
        "        index_to_word[ent]=words#\n",
        "        for w in words:\n",
        "          score+=ent2freq[w]\n",
        "        if score > maxScore:\n",
        "          maxScore, rep = score, ent\n",
        "      final_res[rep] = cluster\n",
        "      cesi_clust2ent_mod2[rep]= wordList\n",
        "      clusterNo+=1\n",
        "final_res_e1=final_res\n",
        "index_e1=[]\n",
        "words_e1=[]\n",
        "for k,v in index_to_word.items():\n",
        "    index_e1.append(k)\n",
        "    words_e1.append(v)\n",
        "    \n",
        "#Gold standard eval\n",
        "def invertDic(my_map, struct = 'o2o'):\n",
        "\tinv_map = {}\n",
        "\tif struct == 'o2o':\t\t\t\t\n",
        "\t\tfor k, v in my_map.items():\n",
        "\t\t\tinv_map[v] = k\n",
        "\telif struct == 'm2o':\t\t\t\n",
        "\t\tfor k, v in my_map.items():\n",
        "\t\t\tinv_map[v] = inv_map.get(v, [])\n",
        "\t\t\tinv_map[v].append(k)\n",
        "\telif struct == 'm2ol':\t\t\t\t\n",
        "\t\tfor k, v in my_map.items():\n",
        "\t\t\tfor ele in v:\n",
        "\t\t\t\tinv_map[ele] = inv_map.get(ele, [])\n",
        "\t\t\t\tinv_map[ele].append(k)\n",
        "\telif struct == 'm2os':\n",
        "\t\tfor k, v in my_map.items():\n",
        "\t\t\tfor ele in v:\n",
        "\t\t\t\tinv_map[ele] = inv_map.get(ele, set())\n",
        "\t\t\t\tinv_map[ele].add(k)\n",
        "\treturn inv_map\n",
        "\n",
        "\n",
        "import itertools, sys\n",
        "\n",
        "def macroPrecision(C_clust2ele, E_ele2clust):\n",
        "\tnum_prec = 0\n",
        "\tfor _, cluster in C_clust2ele.items():\n",
        "\t\tisFirst = True\n",
        "\t\tres = set()\n",
        "\t\tfor ele in cluster:\n",
        "\t\t\tif ele not in E_ele2clust: \n",
        "\t\t\t\tcontinue\n",
        "\t\t\tif isFirst:\n",
        "\t\t\t\tres = E_ele2clust[ele]\n",
        "\t\t\t\tisFirst = False\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tres = res.intersection(E_ele2clust[ele])\n",
        "\t\tif   len(res) == 1: num_prec += 1\n",
        "\tif len(C_clust2ele) == 0: return 0\n",
        "\treturn float(num_prec) / float(len(C_clust2ele))\n",
        "\n",
        "def microPrecision(C_clust2ele, E_ele2clust):\n",
        "\tnum_prec = 0\n",
        "\ttotal = 0\n",
        "\n",
        "\tfor _, cluster in C_clust2ele.items():\n",
        "\t\tfreq_map = {}\n",
        "\t\ttotal += len(cluster)\n",
        "\n",
        "\t\tfor ent in cluster:\n",
        "\t\t\tif ent not in E_ele2clust: \n",
        "\t\t\t\tcontinue\n",
        "\t\t\tfor ele in E_ele2clust[ent]:\n",
        "\t\t\t\tfreq_map[ele] = freq_map.get(ele, 0)\n",
        "\t\t\t\tfreq_map[ele] += 1\n",
        "\t\tmax_rep = 0\n",
        "\t\tfor k, v in freq_map.items(): max_rep = max(max_rep, v)\n",
        "\n",
        "\t\tnum_prec += max_rep\n",
        "\n",
        "\tif total == 0: return 0\n",
        "\treturn float(num_prec) / float(total)\n",
        "\n",
        "def pairPrecision(C_clust2ele, E_ele2clust):\n",
        "\tnum_hit = 0\n",
        "\tnum_pairs = 0\n",
        "\n",
        "\tfor _, cluster in C_clust2ele.items():\n",
        "\t\tall_pairs = list(itertools.combinations(cluster, 2))\n",
        "\t\tnum_pairs += len(all_pairs)\n",
        "\n",
        "\t\tfor e1, e2 in all_pairs:\n",
        "\t\t\tif e1 not in E_ele2clust or e2 not in E_ele2clust:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tres = E_ele2clust[e1].intersection(E_ele2clust[e2])\n",
        "\t\t\tif   len(res) == 1: num_hit += 1\n",
        "\n",
        "\tif num_pairs == 0: return 0\n",
        "\treturn float(num_hit) / float(num_pairs)\n",
        "\n",
        "def pairwiseMetric(C_clust2ele, E_ele2clust, E_clust2ent):\n",
        "\tnum_hit = 0\n",
        "\tnum_C_pairs = 0\n",
        "\tnum_E_pairs = 0\n",
        "\n",
        "\tfor _, cluster in C_clust2ele.items():\n",
        "\t\tall_pairs = list(itertools.combinations(cluster, 2))\n",
        "\t\tnum_C_pairs += len(all_pairs)\n",
        "\n",
        "\t\tfor e1, e2 in all_pairs:\n",
        "\t\t\tif e1 in E_ele2clust and e2 in E_ele2clust and len(E_ele2clust[e1].intersection(E_ele2clust[e2])) > 0: num_hit += 1\n",
        "\n",
        "\tfor rep, cluster in E_clust2ent.items(): \n",
        "\t\tnum_E_pairs += len(list(itertools.combinations(cluster, 2)))\n",
        "\n",
        "\tif num_C_pairs == 0 or num_E_pairs == 0: \n",
        "\t\treturn 1e-6, 1e-6\n",
        "\n",
        "\treturn float(num_hit) / float(num_C_pairs), float(num_hit) / float(num_E_pairs)\n",
        "\n",
        "\n",
        "def calcF1(prec, recall):\n",
        "\tif prec + recall == 0: return 0\n",
        "\treturn 2 * (prec * recall) / (prec + recall)\n",
        "\n",
        "def microF1(C_ele2clust, C_clust2ele, E_ele2clust, E_clust2ent):\n",
        "\tmicro_prec \t= microPrecision(C_clust2ele, E_ele2clust)\n",
        "\tmicro_recall \t= microPrecision(E_clust2ent, C_ele2clust)\n",
        "\tmicro_f1\t= calcF1(micro_prec, micro_recall)\n",
        "\treturn micro_f1\n",
        "\n",
        "def macroF1(C_ele2clust, C_clust2ele, E_ele2clust, E_clust2ent):\n",
        "\tmacro_prec \t= macroPrecision(C_clust2ele, E_ele2clust)\n",
        "\tmacro_recall \t= macroPrecision(E_clust2ent, C_ele2clust)\n",
        "\tmacro_f1\t= calcF1(macro_prec, macro_recall)\n",
        "\treturn macro_f1\n",
        "\n",
        "def pairF1(C_ele2clust, C_clust2ele, E_ele2clust, E_clust2ent):\n",
        "\tpair_prec,pair_recall = pairwiseMetric(C_clust2ele, E_ele2clust, E_clust2ent)\n",
        "\tpair_f1\t\t= calcF1(pair_prec, pair_recall)\n",
        "\treturn pair_f1\n",
        "\n",
        "def evaluate(C_ele2clust, C_clust2ele, E_ele2clust, E_clust2ent):\n",
        "\tmacro_prec \t= macroPrecision(C_clust2ele, E_ele2clust)\n",
        "\tmacro_recall \t= macroPrecision(E_clust2ent, C_ele2clust)\n",
        "\tmacro_f1\t= calcF1(macro_prec, macro_recall)\n",
        "\n",
        "\tmicro_prec \t= microPrecision(C_clust2ele, E_ele2clust)\n",
        "\tmicro_recall \t= microPrecision(E_clust2ent, C_ele2clust)\n",
        "\tmicro_f1\t= calcF1(micro_prec, micro_recall)\n",
        "\n",
        "\tpair_prec,pair_recall = pairwiseMetric(C_clust2ele, E_ele2clust, E_clust2ent)\n",
        "\tpair_f1\t\t= calcF1(pair_prec, pair_recall)\n",
        "\n",
        "\treturn {\n",
        "\t\t'macro_prec': \tround(macro_prec, \t4),\n",
        "\t\t'macro_recall':\tround(macro_recall, \t4),\n",
        "\t\t'macro_f1':\tround(macro_f1, \t4),\n",
        "\n",
        "\t\t'micro_prec': \tround(micro_prec, \t4),\n",
        "\t\t'micro_recall':\tround(micro_recall, \t4),\n",
        "\t\t'micro_f1':\tround(micro_f1, \t4),\n",
        "\n",
        "\t\t'pair_prec': \tround(pair_prec, \t4),\n",
        "\t\t'pair_recall':\tround(pair_recall, \t4),\n",
        "\t\t'pair_f1':\tround(pair_f1, \t\t4),\n",
        "\t}\n",
        "#df_gold = read goldset\n",
        "_id=df_gold['Entity1_ID']\n",
        "true_lnk_sub2=df_gold['true_link_subject']\n",
        "myTrueLink={k:v for k,v in zip(_id,true_lnk_sub2) }\n",
        "myTrueLink2={}\n",
        "count=0\n",
        "for k,v in zip(_id,true_lnk_sub2):\n",
        "    if k in myTrueLink2:\n",
        "      index=count\n",
        "      k=k+0.01\n",
        "      newK=k\n",
        "    myTrueLink2[k]=v\n",
        "    count+=1\n",
        "true_lnk_sub=[]\n",
        "for key in sorted(myTrueLink2):\n",
        "  true_lnk_sub.append(myTrueLink2[key])\n",
        "\n",
        "triples_list=[]\n",
        "dfE1=pd.read_csv(\"/content/gdrive/MyDrive/my Personal work/Open-CyKG/MalwareDB_dataset_csv_exl/KG/Ent1(12March2021).csv\")\n",
        "\n",
        "e1Names=dfE1E['Name']\n",
        "Entity1_ID=dfE1E['Entity1_ID']\n",
        "_id=sorted(myTrueLink2)\n",
        "assert(len(e1Names)==len(_id)==len(true_lnk_sub))\n",
        "for triple_unq, id, goldClust in zip(e1Names,_id,true_lnk_sub):\n",
        "      trp={}\n",
        "      trp['triple_unique']=triple_unq\n",
        "      trp['true_sub_link']=goldClust\n",
        "      triples_list.append(trp)\n",
        "\n",
        "from collections import defaultdict as ddict\n",
        "''' Ground truth clustering: from CESI main '''\n",
        "true_ent2clust = ddict(set)\n",
        "for trp in triples_list:\n",
        "\t\t\tsub_u = trp['triple_unique'] #i think its the subjet ie: entity word (Name)\n",
        "\t\t\ttrue_ent2clust[sub_u].add(trp['true_sub_link']) #true_link_subject\n",
        "true_clust2ent = invertDic(true_ent2clust, 'm2os')  \n",
        "\n",
        "\n",
        "#embedding evaluation from CESI main np_eval function\n",
        "ent_clust=final_res_e1\n",
        "cesi_clust2ent = {}\n",
        "for rep, cluster in ent_clust.items(): \n",
        "  cesi_clust2ent[rep] = set(cluster)\n",
        "cesi_ent2clust = invertDic(cesi_clust2ent, 'm2os')\n",
        "\n",
        "word_to_index=invertDic(index_to_word, 'o2o')\n",
        "true_clust2ent_mod2=ddict(list)\n",
        "\n",
        "for k in true_clust2ent:\n",
        "  first=1\n",
        "  for v in true_clust2ent[k]:\n",
        "    if first:\n",
        "      key=word_to_index[v]\n",
        "    first=0\n",
        "    true_clust2ent_mod2[key].append(v)\n",
        "\n",
        "cesi_ent2clust_mod2 = invertDic(cesi_clust2ent_mod2, 'm2os')\n",
        "true_ent2clust_mod2 = invertDic(true_clust2ent_mod2, 'm2os')  \n",
        "\n",
        "eval_results = evaluate(cesi_ent2clust_mod2, cesi_clust2ent_mod2, true_ent2clust_mod2,true_clust2ent_mod2)\n",
        "\n",
        "print('Macro P: {}, Micro P: {}, Pairwise P: {}'.format(eval_results['macro_prec'], eval_results['micro_prec'], eval_results['pair_prec']))\n",
        "\n",
        "print('Macro R: {}, Micro R: {}, Pairwise R: {}'.format(eval_results['macro_recall'], eval_results['micro_recall'], eval_results['pair_recall']))\n",
        "\n",
        "print('Macro F1: {}, Micro F1: {}, Pairwise F1: {}'.format(eval_results['macro_f1'], eval_results['micro_f1'], eval_results['pair_f1']))\n",
        "print('CESI: #Clusters: %d, #Singletons %d'    % (len(cesi_clust2ent_mod2), \tlen([1 for _, clust in cesi_clust2ent_mod2.items()    if len(clust) == 1])))\n",
        "print('Gold: #Clusters: %d, #Singletons %d \\n' % (len(true_clust2ent_mod2),  len([1 for _, clust in true_clust2ent_mod2.items() if len(clust) == 1])))\n",
        "\n",
        "\n",
        "Macro_r_e1=eval_results['macro_recall']\n",
        "Macro_p_e1=eval_results['macro_prec']\n",
        "Macro_f_e1=eval_results['macro_f1']\n",
        "\n",
        "Micro_r_e1=eval_results['micro_recall']\n",
        "Micro_p_e1=eval_results['micro_prec']\n",
        "Micro_f_e1=eval_results['micro_f1']\n",
        "\n",
        "pw_r_e1=eval_results['pair_recall']\n",
        "pw_p_e1=eval_results['pair_prec']\n",
        "pw_f_e1=eval_results['pair_f1']"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}